{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a245c18-4417-4e95-bb8c-64813156a26a",
   "metadata": {},
   "source": [
    "# Catalog – Working with Distributed HDF/RDF Data\n",
    "\n",
    "The approach proposed by `h5rdmtoolbox` is based on publishing **HDF5 data files together with their semantic metadata** (e.g. RDF/Turtle files). These resources can be hosted on any suitable platform, such as [Zenodo](https://zenodo.org/).\n",
    "\n",
    "## Core idea\n",
    "\n",
    "The concept separates *data storage* from *semantic exploration*:\n",
    "\n",
    "- **HDF5 files** efficiently store large, multidimensional numerical data.\n",
    "- **RDF files** capture semantic metadata; they are lightweight and well suited for querying and exploration.\n",
    "- Users typically **inspect and process the RDF metadata first** (RDF Store), and only **download the corresponding HDF5 files on demand** (HDF Store) when detailed data access is required.\n",
    "\n",
    "## Catalog-driven data selection\n",
    "\n",
    "To define which datasets are relevant for a given context or scope, a **catalog file** is used.  \n",
    "This catalog is provided as a Turtle file and models its information using the  \n",
    "[`dcat:Catalog`](https://www.w3.org/TR/vocab-dcat-3/#Class:Catalog) vocabulary.\n",
    "\n",
    "The catalog acts as an entry point that references all relevant source files (both RDF and HDF5).\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "The diagram below illustrates this workflow:\n",
    "\n",
    "- A `dcat:Catalog` describes and references the available source datasets.\n",
    "- Users interact with the catalog via the `CatalogManager` provided by `h5rdmtoolbox`.\n",
    "- Through this interface, RDF metadata can be queried and processed (RDF Store).\n",
    "- Associated HDF5 data files are downloaded only when needed (e.g. for in-depth analysis) (HDF Store).\n",
    "\n",
    "<div>\n",
    "<img src=\"../../_static/catalog_principle.svg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e4732-99e0-4531-af1e-cc8f8386cfd0",
   "metadata": {},
   "source": [
    "## Define the Scope → dcat:Catalog\n",
    "\n",
    "The catalog’s RDF data defines **which datasets are within the scope** of the current analysis or workflow.  \n",
    "In other words, the `dcat:Catalog` specifies *what data should be considered* and *where it can be found*.\n",
    "\n",
    "The catalog is provided as a Turtle (TTL) file. This file can either be:\n",
    "\n",
    "- **Written manually**, giving full control over the catalog structure and metadata, or\n",
    "- **Generated programmatically** using `ontolutils`, which helps create standards-compliant RDF with less boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4a90e-75b8-491f-a74d-63bf37bafb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontolutils.ex import dcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b05f9-ef05-49c6-824b-28f43b695f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dcat.Catalog(\n",
    "    id=\"https://example.org/tutorial-catalog\",\n",
    "    dataset=dcat.Dataset(\n",
    "        id=\"https://doi.org/10.5281/zenodo.18187577\",\n",
    "        identifier=\"18185973\",\n",
    "        distribution=[\n",
    "            dcat.Distribution(\n",
    "                id=\"https://doi.org/10.5281/zenodo.18187577#random_temperature_data.ttl\",\n",
    "                title=\"random temperature data (metadata)\",\n",
    "                identifier=\"random_temperature_data.ttl\",\n",
    "                downloadURL=\"https://zenodo.org/records/18187577/files/random_temperature_data.ttl\",\n",
    "                mediaType=\"text/turtle\"\n",
    "            ),\n",
    "            dcat.Distribution(\n",
    "                id=\"https://doi.org/10.5281/zenodo.18187577#random_temperature_data.hdf\",\n",
    "                title=\"random temperature data (data)\",\n",
    "                identifier=\"random_temperature_data.hdf\",\n",
    "                downloadURL=\"https://zenodo.org/records/18187577/files/random_temperature_data.hdf\",\n",
    "                mediaType=\"application/x-hdf\"\n",
    "            ),\n",
    "            dcat.Distribution(\n",
    "                id=\"https://doi.org/10.5281/zenodo.18187577#/random_velocity_data.ttl\",\n",
    "                title=\"random temperature velocity (metadata)\",\n",
    "                identifier=\"random_velocity_data.ttl\",\n",
    "                downloadURL=\"https://zenodo.org/records/18187577/files/random_velocity_data.ttl\",\n",
    "                mediaType=\"text/turtle\"\n",
    "            ),\n",
    "            dcat.Distribution(\n",
    "                id=\"https://doi.org/10.5281/zenodo.18187577#random_velocity_data.h5\",\n",
    "                title=\"random velocity data (data)\",\n",
    "                identifier=\"random_velocity_data.hdf\",\n",
    "                downloadURL=\"https://zenodo.org/records/18187577/files/random_velocity_data.h5\",\n",
    "                mediaType=\"application/x-hdf\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(catalog.serialize(\"ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a01fa-d5cc-4391-b70d-5fdbcfc4e1f3",
   "metadata": {},
   "source": [
    "## Instantiate the CatalogManager\n",
    "\n",
    "To instantiate the `CatalogManager`, we first define a **working directory** that is used to store local files and intermediate results.\n",
    "\n",
    "Next, we configure the **RDF store** and the **HDF store**:\n",
    "\n",
    "- For **RDF data**, we use a *local RDF store* based on `rdflib.Graph`.  \n",
    "  This lightweight solution is fully sufficient for the scope of this tutorial.\n",
    "- Alternatively, an external triple store such as **GraphDB** can be used.  \n",
    "  This option offers better performance and scalability for larger catalogs and more complex queries.\n",
    "- The **HDF store** manages access to the referenced HDF5 files and handles downloading them on demand.\n",
    "\n",
    "With these components in place, the `CatalogManager` provides a unified interface for querying RDF metadata and accessing the corresponding HDF5 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1aeb93-a4ec-4b8f-a177-ae2546b38960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.catalog import CatalogManager, InMemoryRDFStore, HDF5FileStore\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef10f05-f51b-4ee4-837e-4390235fc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"local-db\"\n",
    "pathlib.Path(working_dir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad350310-40da-4cdf-a9a3-32fc2a6e017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CatalogManager(\n",
    "    catalog=catalog,\n",
    "    working_directory=working_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c0c65-9881-4642-8790-e0f77cb8ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = InMemoryRDFStore(cm.rdf_directory, formats=\"ttl\")\n",
    "cm.add_main_rdf_store(in_memory_store)\n",
    "cm.download_metadata()\n",
    "cm.main_rdf_store.populate(recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae331c-1b2a-4923-9830-19b883a226d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = HDF5FileStore(data_directory=\"local-db/hdf\")\n",
    "cm.add_hdf_store(data_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5361e9-3df0-4b50-bf5e-820009a6a7ad",
   "metadata": {},
   "source": [
    "Let's check how many triples are loaded to the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6f7df-f3e0-49ce-abb2-e347cc9be02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cm.main_rdf_store.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ca351-36c3-41bc-a25f-c72127344794",
   "metadata": {},
   "source": [
    "## Perform a Query\n",
    "\n",
    "To search the catalog semantically, we define a **SPARQL query** (`SparqlQuery`) and execute it against the RDF store.\n",
    "\n",
    "The query is evaluated on the semantic metadata only, making it lightweight and efficient.  \n",
    "The query result is returned as a **Result** object, which exposes the data as a **pandas DataFrame** (`.data`) for convenient inspection and further processing within the notebook.\n",
    "\n",
    "This allows users to explore and filter available datasets based on their metadata *before* accessing the underlying HDF5 data.\n",
    "\n",
    "In the example below, we search for all subjects that define a unit, using the predicate `m4i:hasUnit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd6fd4-e63b-4b87-a4df-578d30982855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.catalog import SparqlQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027eb6c-f682-4851-8535-1fd4ae01d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SparqlQuery(\n",
    "    query=\"\"\"PREFIX m4i: <http://w3id.org/nfdi4ing/metadata4ing#>\n",
    "\n",
    "SELECT * WHERE {?s m4i:hasUnit ?o}\n",
    "\"\"\",\n",
    "    description=\"Selects all triples with predicate m4i:hasUnit\"\n",
    ")\n",
    "res = query.execute(cm.main_rdf_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26563be5-1a47-4f7c-99a4-930daf0bfcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4ed8f-1f5e-49af-b79f-f6958f86d455",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Case: Inspect a Dataset with a Specific Standard Name\n",
    "\n",
    "Assume that one of the HDF5 datasets in the catalog is annotated with the standard name **`x_velocity`** using the hdf attribute \"standard_name\".  \n",
    "Our goal is to locate this dataset via its semantic metadata and visualize its data.\n",
    "\n",
    "To achieve this, we proceed in two steps:\n",
    "\n",
    "1. **Identify the dataset semantically** by querying the RDF metadata for the given standard name.\n",
    "2. **Access and plot the underlying HDF5 array** once the matching dataset has been found.\n",
    "\n",
    "As a first step, we define a helper function that generates the required SPARQL query.  \n",
    "This function, `find_dataset_with_standard_name`, returns a `SparqlQuery` object tailored to search for datasets with a specific standard name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94dec4-c009-4ef5-b882-77bc259ec75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_with_standard_name(standard_name_str):\n",
    "    query = f\"\"\"PREFIX hdf: <http://purl.allotrope.org/ontologies/hdf5/1.8#>\n",
    "\n",
    "                SELECT ?dataset ?standard_name\n",
    "                WHERE {{\n",
    "                    ?dataset a hdf:Dataset ;\n",
    "                             hdf:attribute ?attribute .\n",
    "                \n",
    "                    ?attribute a hdf:StringAttribute ;\n",
    "                        hdf:data \\\"{standard_name_str}\\\" .\n",
    "                }}\"\"\"\n",
    "    return SparqlQuery(\n",
    "        query=query,\n",
    "        description=f\"Selects dataset with standard name '{standard_name_str}'\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6fed3-b514-4de4-acf9-4b5904ec94b4",
   "metadata": {},
   "source": [
    "Generate and apply the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf578423-e5ee-4be2-9557-e41696a97be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = find_dataset_with_standard_name(\"x_velocity\")\n",
    "res = new_query.execute(cm.main_rdf_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b455b-d723-4b2f-bfb7-1664fbb11a04",
   "metadata": {},
   "source": [
    "We should find exactly one entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f9b69-c453-44b2-8d21-c3c9e0fe394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e0591-8d19-47bc-9036-cfc4508aa66d",
   "metadata": {},
   "source": [
    "Now that we found the HDF5 dataset, we need to identify in which File (distribution) it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fba87-27db-4f4f-a0cc-1eed2d4e7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distribution_based_on_hdf_dataset_iri(hdf_dataset_iri):\n",
    "    query = f\"\"\"PREFIX dcat: <http://www.w3.org/ns/dcat#>\n",
    "                PREFIX hdf:  <http://purl.allotrope.org/ontologies/hdf5/1.8#>\n",
    "                \n",
    "                SELECT ?fileId ?downloadURL\n",
    "                WHERE {{\n",
    "                    ?fileId a hdf:File ;\n",
    "                          dcat:downloadURL ?downloadURL ;\n",
    "                          hdf:rootGroup ?root .\n",
    "                \n",
    "                    ?root (hdf:member)* <{hdf_dataset_iri}> .\n",
    "                }}\"\"\"\n",
    "    return SparqlQuery(\n",
    "        query=query,\n",
    "        description=f\"Finds fileID and downloadURL for hdf dataset iri '{hdf_dataset_iri}'\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9f430-b512-4bab-a3ed-1b07be30d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_url_query = find_distribution_based_on_hdf_dataset_iri(res.data[\"dataset\"][0])\n",
    "download_url_res = distribution_url_query.execute(cm.main_rdf_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31b36b-6b3c-4033-bfb9-6352cdc7edda",
   "metadata": {},
   "source": [
    "## Upload the identified HDF5 file to the HDF5 Store\n",
    "\n",
    "We found the distribution (=hdf file) with its downloadURL. In order to use it we need to register it in the HDF5 Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad0702-9a17-4a6f-b6eb-aecc9c6847ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dist = dcat.Distribution(id=download_url_res.data[\"fileId\"][0], downloadURL=download_url_res.data[\"downloadURL\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264ec29-8980-46b1-b782-98f7a159ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.hdf_store.upload_file(distribution=h5_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65d1f1-dfef-40fa-b8c6-38df36bb0435",
   "metadata": {},
   "source": [
    "## Inspect the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8698c-71c7-4135-8044-df5cf145141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cm.hdf_store.open(h5_dist) as h5:\n",
    "    h5.dump(collapsed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb8cd2-9442-482b-9275-d0215677bf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
