{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2c74a4-1f66-4084-96f5-bdbef565d842",
   "metadata": {},
   "source": [
    "# HDF5 and RDF: FAIR Attributes\n",
    "\n",
    "According to [F1 of the *FAIR Principles*](https://www.go-fair.org/fair-principles/f1-meta-data-assigned-globally-unique-persistent-identifiers/) attributes shall be assigned to globally unique and persistent identifiers.\n",
    "\n",
    "Here's what www.go-fair.org says about it:\n",
    "\n",
    "*\"Globally unique and persistent identifiers remove ambiguity in the meaning of your published data by assigning a unique identifier to every element of metadata and every concept/measurement in your dataset. In this context, identifiers consist of an internet link (e.g., a URL that resolves to a web page that defines the concept such as a particular human protein). Many data repositories will automatically generate globally unique and persistent identifiers to deposited datasets. Identifiers can help other people understand exactly what you mean, and they allow computers to interpret your data in a meaningful way (i.e., computers that are searching for your data or trying to automatically integrate them). Identifiers are essential to the human-machine interoperation that is key to the vision of Open Science. In addition, identifiers will help others to properly cite your work when reusing your data.\"*\n",
    "\n",
    "The *h5rdmtoolbox* allows assigning attributes (and their data) to identifiers. For this, each name and value of an attribute may obtain an IRI (internationalized resource identifier). The following outlines, how it is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af904e-f2a1-48ad-9fa1-41d478c6d44b",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "We can interpret HDF5 objects, their attribute names and attribute values as [RDF triples](https://en.wikipedia.org/wiki/Semantic_triple) (subject-predicate-object), where...\n",
    "- ... a group or dataset is a *subject*\n",
    "- ... the attribute name is a <u>predicate</u>\n",
    "- ... and the attriute value is an **object**\n",
    "\n",
    "In the following, we would like to describe the content of an HDF5 file. There will be a dataset or random data generated by a person, which can be identified/described by a researcher ID (ORCID).\n",
    "\n",
    "We as humans may understand the content of such an HDF5 file. For machines to interpret the data, we need to associate [URIs](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) with the HDF5 objects. In fact, sometimes it may also not very clear to humans, what is meant with a certain attribute. To be unambiguous about it, a URI helps. Think of the attribute \"contact\", we will define. Is it a person or an organization? Note, that URI and [IRI](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier) may be used synonymously - IRI is built on URI by expanding the set of permitted characters.\n",
    "\n",
    "Let's build the example step by step. We start with **creating the group \"contact\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5fa94-ace5-430d-ab79-185fa801f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1dde4c-8783-4be4-9d96-4aa9bf6606e6",
   "metadata": {},
   "source": [
    "## Describing an HDF5 file with persistent metadata\n",
    "\n",
    "### Example part 1: A contact person\n",
    "The file is written by an author. We create a group. It contains all relevant contact data, i.e. the ORCID. The content if the group thus describes the contact person and therefore *is* a person. The group itself, however, gets the predicate *has author*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd9d3c-9597-4135-a55d-6b7262f8901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(mode='w') as h5:\n",
    "    grp = h5.create_group('contact', attrs=dict(orcid='https://orcid.org/0000-0001-8729-0482'))   \n",
    "    grp.rdf.predicate = 'https://schema.org/author'\n",
    "    grp.rdf.type = 'http://xmlns.com/foaf/0.1/Person'  # what the content of group is, namely a foaf:Person\n",
    "    grp.rdf.subject = 'https://orcid.org/0000-0001-8729-0482'  # corresponds to @ID in JSON-LD\n",
    "    grp.rdf.predicate['orcid'] =  'http://w3id.org/nfdi4ing/metadata4ing#orcidId'\n",
    "    grp.attrs['first_name', 'http://xmlns.com/foaf/0.1/firstName'] = 'Matthias'\n",
    "\n",
    "    o = grp.rdf.predicate['orcid']\n",
    "    \n",
    "    h5.dump(collapsed=False)\n",
    "\n",
    "hdf_filename = h5.hdf_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c6169-4b16-47b8-990b-d3bf89eacf66",
   "metadata": {},
   "source": [
    "Using the `rdf` accessory, we can assign the objects (dataset, groups, attributes) with the internationalized resource identifier (IRI). An IRI a web resource and points to the definition in an ontology, e.g. \"contact\" is a \"Person\" and is defined in the ontology FOAF: 'http://xmlns.com/foaf/0.1/Person'. The person \"has a researcher ID\". This predicate is described in the M4i ([metadata4ing](https://nfdi4ing.pages.rwth-aachen.de/metadata4ing/metadata4ing/)) ontology: 'http://w3id.org/nfdi4ing/metadata4ing#orcid'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4342f-7460-4bb1-afe9-6f35158a2a7a",
   "metadata": {},
   "source": [
    "### Assigning metadata to the file rather than the root group\n",
    "\n",
    "If we want to describe the file using attributes, the root group \"/\" is the way to go. However, there we might want to distinguish between the actual file and the root group. For this, we can also use the accessory `frdf`, which allows assigning RDF triples to the file.\n",
    "\n",
    "In the following example, we add the creation date as a root group attribute but explain it as a file attribute rather than a group attribute.\n",
    "\n",
    "Using the method `serialize()` the content is displayed as a Linked Data form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd3c0-110a-4e64-97ff-ad2832304493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "with h5tbx.File(mode='w') as h5:\n",
    "    h5.attrs[\"creation_date\"] = datetime.today()\n",
    "    h5.frdf[\"creation_date\"].predicate = \"http://purl.org/dc/terms/created\"\n",
    "\n",
    "print(h5tbx.serialize(h5.hdf_filename, format=\"ttl\", structural=False, semantic=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43376d9-4a06-49a4-90c0-cc8e0a1172c0",
   "metadata": {},
   "source": [
    "From now on, let's same some work and use the package `namespacelib`, which simplifies the work with the namespaces, so that we don't have to type the full IRI address. Some popular ones are implemented in the `rdflib` package, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae3bf0-24fe-4bb8-ab4e-1e9f3295248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontolutils.namespacelib import M4I, OBO, QUDT_UNIT, QUDT_KIND\n",
    "from rdflib.namespace import FOAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f8a8-7543-4385-bb62-df7f8863c909",
   "metadata": {},
   "source": [
    "As a result, we can type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd206fea-0b24-4c51-b22d-5ec8de9fc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "M4I.orcidId  # equal to http://w3id.org/nfdi4ing/metadata4ing#orcidId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1f54f-6f9e-4efa-8c6b-8aa97ae4f006",
   "metadata": {},
   "source": [
    "### Example part 2: A random data dataset\n",
    "Next, we add the random data dataset with units. We can even describe what type the data is. In our case it shall be velocity data. Without this specification it would otherwise not be clear to the user (or a machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43166d3f-b109-4f75-811f-f23bc5b01112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with h5tbx.File(hdf_filename, mode='r+') as h5:    \n",
    "    ds = h5.create_dataset('grp/random_velocity', data=np.random.random(100))\n",
    "    ds.attrs.create('units',\n",
    "                    rdf_predicate=M4I.hasUnit,\n",
    "                    data='m/s',\n",
    "                    rdf_object=QUDT_UNIT.M_PER_SEC)\n",
    "    ds.attrs.create('quantity_kind',\n",
    "                     data='velocity',\n",
    "                     rdf_predicate=M4I.hasKindOfQuantity,\n",
    "                     rdf_object=QUDT_KIND.Velocity)\n",
    "\n",
    "    h5.dump(collapsed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809b75b-8ab2-41fa-ae60-2189df38955e",
   "metadata": {},
   "source": [
    "Now, let's go further and describe how the random dataset was created and that the contact was involved in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17648b2-7a84-4486-8e73-613d05d03f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "with h5tbx.File(hdf_filename, mode='r+') as h5:  \n",
    "    proc = h5.create_group('processing_info')\n",
    "    proc.rdf.subject = M4I.ProcessingStep\n",
    "    proc.attrs['has_participants', OBO.has_participant] = h5['contact']\n",
    "    start_time = datetime.today()\n",
    "    end_time = datetime.today()\n",
    "    proc.attrs.create('start_time', data=start_time,\n",
    "                      rdf_predicate='https://schema.org/startTime')\n",
    "    proc.attrs.create('end_time', data=end_time,\n",
    "                      rdf_predicate='https://schema.org/startTime')\n",
    "    proc.attrs['output', 'http://purl.obolibrary.org/obo/RO_0002234'] = h5['grp/random_velocity'].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e6b44-f6b7-4717-bbf2-e34143e9366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tbx.dump(hdf_filename, collapsed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086c824-8ace-4591-989b-153628752646",
   "metadata": {},
   "source": [
    "### Example part 3: Assigning JSON-LD to describe data\n",
    "\n",
    "Until now, we used IRIs to assign meaning to HDF5 attributes, e.g. `proc.rdf.subject = M4I.ProcessingStep`.\n",
    "\n",
    "Sometimes, data cannot be expressed by a single IRI, because there is no globally unique identifier. Let's examine this case by using the [SSNO](https://matthiasprobst.github.io/ssno/) Ontology.\n",
    "\n",
    "In the example below, the attribute \"standard_name\" of the dataset \"u\" refers to \"x_velocity\" being the [Standard name](https://matthiasprobst.github.io/ssno#StandardName) of the HDF5 dataset \"u\". A Standard name has a name, description and SI unit and may be associated to a Standard Name Table in which it is listed. In our case, the Standard name \"x_velocity\" has no globally unique identifier, hence we need to describe it by a JOSN-LD string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527f333-a685-44f4-bb2c-0a55e48b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_xvel = \"\"\"{\n",
    "    \"@context\": {\n",
    "        \"ssno\": \"https://matthiasprobst.github.io/ssno#\"\n",
    "    },\n",
    "    \"@type\": \"ssno:StandardName\",\n",
    "    \"ssno:standardName\": \"x_velocity\",\n",
    "    \"ssno:unit\": \"http://qudt.org/vocab/unit/M-PER-SEC\",\n",
    "    \"ssno:description\": \"X-component of a velocity vector.\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5e71e-e0f4-4142-990b-083ce09b4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    h5.create_dataset(\"u\", data=[1,2,3], attrs={\"standard_name\": \"x_velocity\"})\n",
    "    h5.u.rdf[\"standard_name\"].predicate = \"https://matthiasprobst.github.io/ssno#hasStandardName\"\n",
    "    # h5.u.rdf[\"standard_name\"].object = sn_xvel\n",
    "    h5.u.rdf[\"standard_name\"].object = sn_xvel\n",
    "    h5.dump(False)\n",
    "    h5jld = h5.dump_jsonld(indent=2, structural=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc56bfd-510c-44c5-a910-b9203235d898",
   "metadata": {},
   "source": [
    "The JSON-LD dump shows that \"standard_name\" is correctly associated with our JSON-LD string for the `ssno:StandardName`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e53c0-2348-403a-93e0-21e055463292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h5jld)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c6bb4-5e7f-499d-9a4a-f3c18073b125",
   "metadata": {},
   "source": [
    "## How to make use of the FAIR HDF5 file?\n",
    "\n",
    "There are three ways, how the above IRI assignments help us and how we might want to use the information:\n",
    "1. Visual inspection by dumping the content to screen: This will outline the file (meta) content and we can click on the attributes with IRIs, which will explain the attribute (data)\n",
    "2. We can extract a *JSON-LD* file. This is useful for other processes. We can also investigate this file further with tools like [JSON-LD-playground](https://json-ld.org/playground/).\n",
    "3. Access IRI in (Python) code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2de9ad-ac05-47af-a44e-27e257759a66",
   "metadata": {},
   "source": [
    "### 1. Visual inspection\n",
    "\n",
    "The *dump()* method will now add IRI-icons. Click on it and get redirected to the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb9b55-768b-4331-a42f-89bad556abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tbx.dump(hdf_filename, collapsed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b8cc2-bc67-42d1-8f5b-ebe1b08161fe",
   "metadata": {},
   "source": [
    "### 2. JSON-LD extraction\n",
    "\n",
    "Write the JSON-LD or Turtle (ttl) file and share it with others or a repository. The toolbox provides `dump`-methods through the `jsonld` module or - in the newer version of h5tbx - the `serialize` method, allowing to write various linked data formats.\n",
    "\n",
    "It might look a bit overwelming, however dedicated scripts can perfectly work with it while humans still can read it (with a bit of practice and patience...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec944a41-6988-4c1a-9caf-da12eb4856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    h5tbx.serialize(\n",
    "        hdf_filename,\n",
    "        format=\"ttl\",\n",
    "        indent=2,\n",
    "        context={'m4i': 'http://w3id.org/nfdi4ing/metadata4ing#',\n",
    "                 'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "                 'obo': 'http://purl.obolibrary.org/obo/'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89a8fe-0d8e-4a49-8627-1b887f04d152",
   "metadata": {},
   "source": [
    "## 3. Access IRI in code\n",
    "\n",
    "You may want to access the IRI of an attribute with Python within the HDF5 file. E.g. while working with the file, you may ask \"Hey, what is 'contact' exactly?\" or \"What does the attribute 'orcid' mean?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a35de3-b332-4e4f-b52d-324e9f02465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(hdf_filename) as h5:\n",
    "    person_iri = h5.contact.rdf.subject\n",
    "    orcid_iri = h5.contact.rdf.predicate['orcid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb7ca2-ce0a-44c0-843b-1bfb7225d58c",
   "metadata": {},
   "source": [
    "... Well \"contact\" is a \"Person\" defined by the FOAF ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af4e53-a34b-4140-9e26-7205183b776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_iri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa246cf-1d3e-440e-b680-ee421cab5fa4",
   "metadata": {},
   "source": [
    "... and \"orcid\" is a predicate defined by the metadata4ing ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5daa0d-c51b-4331-b53b-db916005fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid_iri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e4bb6-d3f9-4e33-b94c-51753c8bede4",
   "metadata": {},
   "source": [
    "### 3.1 Find data based on IRIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59600274-baf3-42b8-8f82-da8fb705bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib.graph as g\n",
    "\n",
    "graph = g.Graph()\n",
    "graph.parse('hdf_meta.jsonld', format='json-ld')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fe2fa-dc03-4ef4-bffc-f7cead97c99d",
   "metadata": {},
   "source": [
    "Note, that we need to provide the PREFIXES, if the json-ld data/file does not include the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3ca6e-475c-4ef2-87f9-9794993dfe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = graph.query(\"\"\"\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "PREFIX m4i: <http://w3id.org/nfdi4ing/metadata4ing#>\n",
    "\n",
    "SELECT ?id ?orcid\n",
    "WHERE {\n",
    "    ?id a foaf:Person .\n",
    "    ?id m4i:orcid ?orcid .\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dcaf7-3c38-45d2-9bda-5e207f03bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ffc5e-48a4-4176-b834-5b6b9b48053e",
   "metadata": {},
   "source": [
    "## 4. Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d93209-493f-46ee-bbfc-3373a0cd7cba",
   "metadata": {},
   "source": [
    "### 4.1 Read metadata from JSON and write to HDF5\n",
    "\n",
    "Suppose we want to store information about the used software to the HDF5 file. It exists as a JSON-LD file based on the codemeta ontolog. For this example, we use the *h5rdmtoolbox* codemeta.json file from the github repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca4659-0d68-4965-9059-7ebafda010f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.utils import download_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2937d00-dc89-4bc5-8fb2-913d2197c00e",
   "metadata": {},
   "source": [
    "Download the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21788a70-ce78-4262-b7c4-553df8f85981",
   "metadata": {},
   "outputs": [],
   "source": [
    "codemeta_url = 'https://raw.githubusercontent.com/matthiasprobst/h5RDMtoolbox/main/codemeta.json'\n",
    "dowloaded_filename = download_file(codemeta_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1b74d-6ca3-4632-b665-b67a6245ccd4",
   "metadata": {},
   "source": [
    "Read the data with `ontolutils.dquery`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20c8fd-1075-4575-88da-c85e6af81a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontolutils import dquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654415a2-ea18-4d43-b692-f5db58f2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dquery(subject='schema:SoftwareSourceCode',\n",
    "              source=dowloaded_filename,\n",
    "              context={\"schema\": \"http://schema.org/\"})\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5fb8f-49d3-4da2-b087-16bcdf4b06f0",
   "metadata": {},
   "source": [
    "The data are written into the HDF5 file by using `jsonld.to_hdf()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43f95b-4afb-42cd-bf75-a7ee6536cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.wrapper import jsonld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed644e6-8ddd-4dc3-9eea-c83fa7825cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File('test.hdf', 'w') as h5:\n",
    "    jsonld.to_hdf(data=data[0],\n",
    "                 grp=h5.create_group('software_code'))\n",
    "    h5.dump(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc299a-efb9-4d5a-be58-dcff2f34d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ontolutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ff8c4-06b8-4a32-9875-52a982eef604",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(mode='w') as h5:\n",
    "    _ = h5.create_dataset('test_dataset', data=np.array([[1, 2], [3, 4], [5.4, 1.9]]))\n",
    "    h5.create_dataset('grp/subgrp/vel', data=4)\n",
    "    h5.attrs['name', ontolutils.SCHEMA.name] = 'test attr'\n",
    "\n",
    "    ttl = h5tbx.serialize(h5.filename, structural=True, format=\"ttl\")\n",
    "print(ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1e809-4d41-448b-8034-f4674949cba2",
   "metadata": {},
   "source": [
    "## Describing attribute meanings without RDF\n",
    "\n",
    "Sometimes, there is no IRI (yet) defined but the need to give an additional comment on the attribute. This can be done by as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fb92a-7ba0-455a-9f92-8a6a959ec5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    grp = h5.create_group('contact')\n",
    "\n",
    "    # Set an attribute as usual\n",
    "    grp.attrs['type'] = 'Contact'\n",
    "\n",
    "    # Update the attribute definition afterwards:\n",
    "    grp.rdf['type'].definition = 'The role of the Person'\n",
    "\n",
    "    # Alternatively, it can be assigned simultaneously via h5tbx.Attribute:\n",
    "    grp.attrs['fname'] = h5tbx.Attribute(value='Matthias',\n",
    "                                        definition='The first name of the contact')\n",
    "    h5.dump(False)\n",
    "\n",
    "    jdict = h5.dump_jsonld(h5.hdf_filename, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee950caf-29cf-4c40-b12d-ef3ad109df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c065f00-be16-4f6e-9108-3a98e9885a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
