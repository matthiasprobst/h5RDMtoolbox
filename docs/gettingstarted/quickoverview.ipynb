{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c6a94-d44e-4b89-bcb2-56a913faaafb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Quick Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30cac8-1a28-46e1-b886-8b10979c4098",
   "metadata": {},
   "source": [
    "This chapter gives a quick overview into how to use the package. Detailed explanations can be found in the [userguide](../userguide/index.rst).\n",
    "\n",
    "Start by importing the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d821ed-27db-46eb-9206-435ac5208f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c93e7-6d5f-4187-896e-f8223d0449c3",
   "metadata": {},
   "source": [
    "## Difference to `h5py` package\n",
    "\n",
    "The `h5RDMtoolbox` is built upon the `h5py` package. The base functionality is kept, but convenient features and interfaces are added.\n",
    "\n",
    "### Filename\n",
    "A filename must not be provided when creating a new file. If none is provided, a temporary file is created. Also, `hdf_filename` is provided as an additional property allowing to work with the filename even after the file has been closed *and* to work with `pathlib.Path` objects instead of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d270b-6c02-4296-8ec7-34dc05af90df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.use(None):\n",
    "    with h5tbx.File() as h5:\n",
    "        pass\n",
    "h5.hdf_filename.name  # equal to h5.filename but a pathlib.Path and exists also after the file is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37608c-aba1-4823-a8ec-b2e7897d559c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset features\n",
    "\n",
    "The toolbox aims to simplify working with HDF5 files. You can experience this some functionality being added during object creation, e.g. it is possible to create attributes during group and dataset creations.\n",
    "\n",
    "Also the inspection of the file content is made very easy and user friendly.\n",
    "\n",
    "Below we create a HDF5 dataset with two attributes and dump the content. Use `dump()` in Notebooks and `dumps()` in scripts or a python console. Note, how the file representation is interactive. Also note, that there is even more interactivity when we explore the [RDF feature](#Semantification-of-HDF-data-using-RDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091e1bb-f1d3-400a-9833-35866416d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    ds_time = h5.create_dataset(\n",
    "        name='time',\n",
    "        data=[0, 1, 2, 3],\n",
    "        attrs=dict(units='s', long_name='measurement time'),\n",
    "        make_scale=True\n",
    "    )\n",
    "    h5.dump(collapsed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228862d-8150-40fb-af60-0d46625dcada",
   "metadata": {},
   "source": [
    "### Datasets/xarray interface\n",
    "\n",
    "Data access will not return `np.ndarray` but a `xr.DataArray` object. It is capable of storing attributes and coordinates (similar concept as HDF dimension scales). Find out about all possibilities this give on [xarray's documentation](https://xarray.pydata.org/).\n",
    "\n",
    "Let's create some sample data and see how this new return object can help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69936b5e-d89b-47e5-868c-2121f55b264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time = np.linspace(0, np.pi/4, 21) # units [s]\n",
    "signal = np.sin(2*np.pi*3*time) # units [V], physical: [m/s]\n",
    "\n",
    "with h5tbx.File() as h5:\n",
    "    vel_hdf_filename = h5.hdf_filename # store for later use\n",
    "    \n",
    "    ds_time = h5.create_dataset(name='time',\n",
    "                                data=time,\n",
    "                                attrs=dict(units='s',\n",
    "                                           long_name='measurement time'),\n",
    "                                make_scale=True)\n",
    "    \n",
    "    ds_signal = h5.create_dataset(name='vel',\n",
    "                                  data=signal,\n",
    "                                  attrs=dict(units='m/s',\n",
    "                                             long_name='air velocity in pipe'),\n",
    "                                  attach_scale=ds_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e7a94-ac37-4941-9b29-8343d3884e2c",
   "metadata": {},
   "source": [
    "Inspired by `xarray` the methods `sel` and `isel` are implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0de69-c4b1-4acb-8155-70ef82bd6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    vel2 = h5['vel'].sel(time=2, method='nearest')\n",
    "vel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c5ae5-a0c7-4b7b-955f-7ba05246be0c",
   "metadata": {},
   "source": [
    "Another advantage is using the plotting util form `xarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b486d-4adc-47ce-89ac-42ff6c2f3d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    vel_data = h5['vel'][:]\n",
    "    vel_data.plot(marker='o')\n",
    "    \n",
    "vel_data  # this returns the interactive view of the array and its meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627e47a-37d1-45b2-8adc-d04d038b1574",
   "metadata": {},
   "source": [
    "### Natural Naming\n",
    "Until here, we used the conventional way of addressing variables and groups in a dictionary-like style. `h5RDMtoolbox` allows using \"natural naming\" which means that we can address those objects as if they were attributes. Make sure `h5tbx.config.natural_naming` is set to `True` (the default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b678a5-403f-4055-9d1b-45153dcb7b58",
   "metadata": {},
   "source": [
    "Let's first disable `natural_naming`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c1b74-bd60-4c2e-b498-1f3dab56aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.set_config(natural_naming=False):\n",
    "    with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "        try:\n",
    "            ds = h5.vel[:]\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514ae51-07ca-4e97-a37f-bbd3a8c29f3d",
   "metadata": {},
   "source": [
    "Enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a088543-0338-4022-a691-4108a95b1951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.set_config(natural_naming=True):\n",
    "    with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "        ds = h5.vel[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114e99-7d0e-4b6f-8b41-7c6e8e76b78b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Conventions\n",
    "The file content is controlled by means of a `convention`. This means that specific attributes are required for HDF groups or datasets.\n",
    "\n",
    "They can be understood as rules, which are validated during usage. To make those rules to become effective, the convention must be imported and enabled. Conventions can be created by the user, too. More on this [here](../userguide/convention/index.rst).\n",
    "\n",
    "For now, we select the existing one, which is published on [Zenodo](https://zenodo.org/record/10428795)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bb0fe-9187-4350-9c1e-1fe117613dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.repository.zenodo import ZenodoRecord\n",
    "\n",
    "cv = h5tbx.convention.from_repo(\n",
    "    ZenodoRecord(source=12526361),\n",
    "    name=\"tutorial_convention.yaml\"\n",
    ")\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073571b-0284-487f-9758-4bff9e9891d6",
   "metadata": {},
   "source": [
    "From the above representation string of the convention object we can read which attributes are *optional* or **required** for file creation (`__init__`), dataset creation (`create_dataset`) or group creation (`create_group`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3c0f6-4a1d-452a-85c6-1731a42b0bf4",
   "metadata": {},
   "source": [
    "Without enabling the convention, the working with HDF5 files through the `h5rdmtoolbox` is almost (we got a few additional features which make life a bit easier) as by using `h5py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd20b3-62c7-4c00-bcec-76b937c9fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa06687-c9c0-4a20-af30-91f337fb09d5",
   "metadata": {},
   "source": [
    "**Now, we enable the convention ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb2a60-752c-4d14-844e-16841a060d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tbx.use(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071903e-e60c-45c6-a4e5-758533975171",
   "metadata": {},
   "source": [
    "... and get an error, because we are not providing a \"data_type\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5049224-a978-4baf-ada6-70b8ede3dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with h5tbx.File() as h5:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcea207-3156-48bb-b13e-719bc1849128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time = np.linspace(0, np.pi/4, 21) # units [s]\n",
    "signal = np.sin(2*np.pi*3*time) # units [V], physical: [m/s]\n",
    "\n",
    "with h5tbx.File(contact=h5tbx.__author_orcid__, data_type='experimental') as h5:\n",
    "    vel_hdf_filename = h5.hdf_filename # store for later use\n",
    "    \n",
    "    ds_time = h5.create_dataset(name='time',\n",
    "                                data=time, \n",
    "                                units='s',\n",
    "                                long_name='measurement time',\n",
    "                                make_scale=True)\n",
    "    \n",
    "    ds_signal = h5.create_dataset(name='vel',\n",
    "                                  data=signal,\n",
    "                                  units='m/s',\n",
    "                                  long_name='air velocity in pipe',\n",
    "                                  attach_scale=ds_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e0c62-957c-46ad-95ed-a75800d5a47f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Semantification of HDF data using RDF\n",
    "\n",
    "The files can be described by RDF triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4e09b-c32d-4f5b-9f6f-713e4ea220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4421b57-b1f4-40c1-b86b-c8aa1ce4369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tbx.use(None)\n",
    "\n",
    "with h5tbx.File() as h5:\n",
    "    # use `frdf` for \"file attributes\":\n",
    "    h5.attrs[\"title\"] = \"Test file\"\n",
    "    h5.frdf[\"title\"].predicate = rdflib.DCTERMS.title\n",
    "    h5.frdf[\"title\"].object = [rdflib.Literal(\"Test file\", \"en\"), rdflib.Literal(\"Test Datei\", \"de\")]\n",
    "\n",
    "    h5.attrs[\"created\"] = \"2025-05-05\"\n",
    "    h5.frdf[\"created\"].predicate = \"http://purl.org/dc/terms/created\"\n",
    "    \n",
    "    grp = h5.create_group('contact', attrs=dict(orcid='https://orcid.org/0000-0001-8729-0482'))   \n",
    "    grp.rdf.type = 'http://xmlns.com/foaf/0.1/Person'  # what the content of group is, namely a foaf:Person\n",
    "    grp.frdf.file_predicate = rdflib.DCTERMS.creator  # the creator shall be related to the file\n",
    "    grp.rdf.subject = 'https://orcid.org/0000-0001-8729-0482'  # corresponds to @ID in JSON-LD\n",
    "    \n",
    "    grp.rdf['orcid'].predicate =  'http://w3id.org/nfdi4ing/metadata4ing#orcidId'\n",
    "    \n",
    "    grp.attrs['first_name', rdflib.FOAF.givenName] = 'Matthias'\n",
    "    grp.attrs['last_name', rdflib.FOAF.familyName] = 'Probst'\n",
    "\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833ff03-c6c9-46ac-a993-6c008e06a219",
   "metadata": {},
   "source": [
    "One of the benefits is that the user can understand the meaning of the data. A machine-interpretable and standardised common exchange file format used by Semantic Web technology is JSON-LD. The toolbox also allows exporting to this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab3f88-c075-42ac-80d2-471fc5cb49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h5tbx.serialize(h5.hdf_filename, format=\"ttl\", file_uri=\"https://example.org#\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fcc75-2b42-4175-bef5-68be2334ef9d",
   "metadata": {},
   "source": [
    "### Validation with SHACL\n",
    "\n",
    "Being able to generate RDF data means, that we could also use [SHACL](https://www.w3.org/TR/shacl/) to validate files.\n",
    "\n",
    "The following SHACL shape defines, that each `hdf:File` must have a creation date specified via `dcterms:created`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d096995-e236-40f1-bbd1-8938dcfc82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shacl_shape = '''@prefix sh: <http://www.w3.org/ns/shacl#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix hdf: <http://purl.allotrope.org/ontologies/hdf5/1.8#> .\n",
    "@prefix ex: <http://example.org/ns#> .\n",
    "\n",
    "ex:HDFFileCreatedShape\n",
    "    a sh:NodeShape ;\n",
    "    sh:targetClass hdf:File ;                # apply only to hdf:File instances\n",
    "    sh:property [\n",
    "        sh:path dcterms:created ;            # must have this property\n",
    "        sh:datatype xsd:date ;               # value must be a date\n",
    "        sh:minCount 1 ;                      # at least one occurrence\n",
    "        sh:maxCount 1 ;                      # optional but recommended\n",
    "        sh:message \"Each hdf:File must have exactly one dcterms:created value of type xsd:date.\" ;\n",
    "    ] .\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d78d6-1d6b-4858-820a-64591458bfd7",
   "metadata": {},
   "source": [
    "The toolbox provides the validation function via the module `ld` (linked-data). In our case, the validation succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25ed4a-358f-4e71-82d8-b8c3ae5be7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.ld.shacl import validate_hdf\n",
    "\n",
    "res = validate_hdf(\n",
    "    hdf_source=h5.hdf_filename,\n",
    "    shacl_data=shacl_shape\n",
    ")\n",
    "print(res.results_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026565ee-10c0-4599-89dc-f199dd50b4f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Databases\n",
    "\n",
    "The `h5rdmtoolbox` has currently implemented two solutions to use databases with HDF5 file. One solution is mapping metadata into a [mongoDB](https://www.mongodb.com/) database. The other uses the HDF5 file itself as a database and allows querying without any further step.\n",
    "\n",
    "In this quick tutorial, we use the second solution. More on the topic can be found in the [documentation](https://h5rdmtoolbox.readthedocs.io/en/latest/database/index.html)\n",
    "\n",
    "Let's find the dataset with name \"/vel\" (yes, trivial in this case, but just to get an idea). We use `find_one`, because we want to find only one (the first) occurrence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3158ff0-0edf-416b-b07a-c6269c91b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.database import FileDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6af1b-a13b-49ee-996c-825a2ced8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = FileDB(vel_hdf_filename).find_one({'$name': '/vel'})\n",
    "print(res.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfd9f0-169e-4300-bf68-2a169e270cff",
   "metadata": {},
   "source": [
    "The same can be done from an opened file, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d13a5e-7267-4787-971e-eddca4f47ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    res = h5.find_one({'$name': '/vel'})\n",
    "res.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a9eb5-85fb-48e0-adfc-9141a0f0e0ab",
   "metadata": {},
   "source": [
    "Let's find all (`find`) datasets with the attribute \"units\" and any value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f818a6-b087-4880-9c42-e65a6bfd943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = FileDB(vel_hdf_filename).find({'units': {'$regex': '.*'}})\n",
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88c46-3640-46fa-86f1-4a2b92fef688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Layouts\n",
    "\n",
    "Layouts define how a file is expected to be organized, which groups and datasets must exist, which attributes are expected and much more. Layout define expectations and thus help file exchange where multiple users are involved. In the jargon of the toolbox, we call these \"specifications\".\n",
    "\n",
    "*Note*: In future versions of the toolbox, the layout definition may be formulated as a SHACL shape, however this is less pythonic and more difficult to write, since it needs knowledge of the SHACL syntax.\n",
    "\n",
    "**Design concept**<br>\n",
    "The module *layouts* makes use of the database solution for HDF5 files. The idea is, that we should be able to formulate our expectations/specifications in the form of a query. For more detailed information, see [here](https://h5rdmtoolbox.readthedocs.io/en/latest/layouts.html). So we write down our queries, which we expect to find HDF5 objects in a file, when we validate one in the future.\n",
    "\n",
    "Let's design a simple one, which requires all datasets to have the attribute \"units\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aae76a-0e2a-4adc-9522-6707cc080bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.layout import Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9284d79-3462-45ef-b445-bdedf5a12387",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay = Layout()\n",
    "\n",
    "spec_all_dataset = lay.add(\n",
    "    FileDB.find,  # query function\n",
    "    flt={},\n",
    "    objfilter='dataset',\n",
    "    n=None\n",
    ")\n",
    "\n",
    "# The following specification is added to the previous.\n",
    "# This will apply the query only on results found by the previous query\n",
    "spec_compression = spec_all_dataset.add(\n",
    "    FileDB.find,\n",
    "    flt={'units': {'$exists': True}}, # attribute \"units\" exists\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# we added one specification to the layout. let's check:\n",
    "lay.specifications  # note, that the second specification is not shown, because it is part of the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3408c38-a67a-476f-818c-5ac139eeb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lay.validate(vel_hdf_filename)\n",
    "res.is_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9731be-02c6-4a84-bfee-cdc3c9257ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.print_summary(exclude_keys=('kwargs', 'target_name', 'target_type'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724c12c-4b0b-40c3-9986-7f53c8bc2d37",
   "metadata": {},
   "source": [
    "The above layout successfully validate the file.\n",
    "\n",
    "Now, let's add the specification:\n",
    "- The file must have one dataset named \"pressure\".\n",
    "- The exact location within the file does not play a role.\n",
    "- This specific dataset must have the unit \"Pa\":\n",
    "- The shape of the dataset must be equal to (21, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c4bea-4396-48a1-8514-40c761c4d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay.add(\n",
    "    FileDB.find_one,  # query function\n",
    "    flt={'$name': {'$regex': 'pressure'}, \n",
    "         '$shape': (21, ),\n",
    "         'units': 'Pa'},\n",
    "    objfilter='dataset',\n",
    "    n=1\n",
    ")\n",
    "lay.specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9a40a-90bf-469f-97b3-7e9fa0f5aa04",
   "metadata": {},
   "source": [
    "The validation now fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb6954-4b82-46a4-ba23-65f716524861",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lay.validate(vel_hdf_filename)\n",
    "res.is_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7764cb0-cb3b-41cc-b681-ab2fd0d894ea",
   "metadata": {},
   "source": [
    "Let's add such a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537c483-9c2d-4361-9ed8-371df1574314",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename, 'r+') as h5:\n",
    "    h5.create_dataset('subgrp/pressure', shape=(21,), attrs={'units': 'Pa'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271037d-d0d4-482b-8ce0-beb6e7f0f0e6",
   "metadata": {},
   "source": [
    "And perform the validation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572d50e-9f4d-4c38-aa4e-d47a9b682c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lay.validate(vel_hdf_filename)\n",
    "res.is_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575c1a3-f194-4959-9ebc-1fc960056e6c",
   "metadata": {},
   "source": [
    "Feel free to play with the layout specifications and the HDF5 file content. For sure, knowledge about performing queries with the used database is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b7854-339f-4fa0-b1f2-03e74f77d07e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Repositories\n",
    "\n",
    "Finally, we can publish our data. The toolbox has implemented an interface to [Zenodo](https://zenodo.org/). Using it with the sandbox (testing) environment requires an API TOKEN. For this, please provide the environment variable \"ZENODO_SANDBOX_API_TOKEN\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c1e6f-9de1-4abf-a86e-cd61486242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %set_env ZENODO_SANDBOX_API_TOKEN=<your token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bb708-ee10-430b-aa25-a4f2a971df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.repository import zenodo\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc27e17-25ab-4566-b247-e00964a8cd7d",
   "metadata": {},
   "source": [
    "Create a new deposit (repo in the testing environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706416a-e19b-46ec-b0cd-6a9c04b2eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit = zenodo.ZenodoRecord(None, sandbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1deefa4-b487-49ae-a61e-440e37507abb",
   "metadata": {},
   "source": [
    "Prepare metadata according to the Zenodo API: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ef32e-e68b-4dc8-935a-bc002dfbdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = zenodo.metadata.Metadata(\n",
    "    version=\"1.0.0\",\n",
    "    title='H5TBX Quick Overview Test',\n",
    "    description=f'The file created in the quick overview script using the h5rdmtoolbox version {h5tbx.__version__}.',\n",
    "    creators=[zenodo.metadata.Creator(name=\"Probst, Matthias\",\n",
    "                                      affiliation=\"Karlsruhe Institute of Technology, Institute for Thermal Turbomachinery\",\n",
    "                                      orcid=\"0000-0001-8729-0482\")],\n",
    "    upload_type='dataset',\n",
    "    access_right='open',\n",
    "    keywords=['h5rdmtoolbox', 'tutorial', 'repository-test'],\n",
    "    publication_date=datetime.now(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f2ed2-96d4-4a26-a0e2-809dec839e8b",
   "metadata": {},
   "source": [
    "push metadata to the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dbdf4-13f9-4364-beda-0184ff9de99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit.metadata = meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c4fe0-d1b1-4baa-bd3a-395a46f502b0",
   "metadata": {},
   "source": [
    "**Upload the HDF5 file:**<br>\n",
    "As HDF5 files cannot be previewed in the repository web interface and HDF5 files may become very large, it is handy to upload a metadata file as an additional resource. This allows to preview the metadata content before downloading the larger HDF5 file.\n",
    "\n",
    "This functionality is given by the upload method `.upload_file`, which takes the optional parameter `metamapper`. Please provide a function which creates a file with metadata. The toolbox comes with a solution, that maps the HDF5 metadata content into a JSON-LD file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24029142-a2ff-4417-8aca-6bc614355abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox import jsonld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba875bb7-40da-4938-bb6c-74aeebce2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit.upload_file(filename=vel_hdf_filename, metamapper=jsonld.hdf2jsonld, skipND=1) # skipND is needed by hdf2jsonld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788568e-caec-4558-b41b-e38e58951aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
